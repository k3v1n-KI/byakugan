{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d777de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import asyncio\n",
    "from playsound import playsound\n",
    "import cv2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "import openai\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from text_to_speech import text_to_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747c739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e117b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476d24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "\"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\",\n",
    "\"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \n",
    "\"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "\"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
    "\"ball\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\",\n",
    "\"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\",\n",
    "\"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\",\n",
    "\"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "\"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
    "\"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\", \"watch\", \"game controller\", \"Sword\", \"wand\", \"speaker\"]\n",
    "# buffer for summary data\n",
    "scene_buffer = []\n",
    "buffer_lock  = threading.Lock()\n",
    "last_send    = time.time()\n",
    "SEND_INTERVAL = 7  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d689f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_gpt():\n",
    "    global last_send\n",
    "    while True:\n",
    "        now = time.time()\n",
    "        if now - last_send >= SEND_INTERVAL:\n",
    "            # 1) grab & clear the buffer in one quick lock\n",
    "            with buffer_lock:\n",
    "                if scene_buffer:\n",
    "                    batch = scene_buffer.copy()\n",
    "                    scene_buffer.clear()\n",
    "                else:\n",
    "                    batch = None\n",
    "                last_send = now\n",
    "\n",
    "            # 2) only if we actually had data, call OpenAI **outside** the lock\n",
    "            if batch:\n",
    "                prompt = \"You are an assistant for a visually impaired user. \"\\\n",
    "                         \"Here is what was just seen:\\n\"\n",
    "                for evt in batch:\n",
    "                    prompt += (\n",
    "                        f\"- {evt['class']} (ID {evt['track_id']}) at \"\n",
    "                        f\"x={evt['cx']:.0f}, y={evt['cy']:.0f} moving \"\n",
    "                        f\"vx={evt['vx']:.1f}, vy={evt['vy']:.1f}\\n\"\n",
    "                    )\n",
    "                prompt += \"\\nPlease describe this briefly in natural spoken language.\"\n",
    "\n",
    "                try:\n",
    "                    resp = openai.chat.completions.create(\n",
    "                        model=\"gpt-4\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\",\n",
    "                             \"content\": \"You describe surroundings for a blind user.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=0.7,\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    text = resp.choices[0].message.content.strip()\n",
    "                    print(f\"\\n[Descriptive update]\\n{text}\\n\")\n",
    "                    loop  = asyncio.get_event_loop_policy().get_event_loop()\n",
    "                    try:\n",
    "                        output_file = \"output_audio.mp3\"\n",
    "                        \n",
    "                        # Run the text_to_speech function\n",
    "                        loop.run_until_complete(text_to_speech(text, output_file))\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "                    finally:\n",
    "                        loop.close()\n",
    "                        # Play the audio file after conversion\n",
    "                        playsound(output_file)\n",
    "                        os.remove(output_file) if os.path.exists(output_file) else None\n",
    "                        print(\"Temporary audio file removed.\")\n",
    "                except Exception as e:\n",
    "                    # don’t let the thread die—just log the error\n",
    "                    print(f\"[GPT Error] {e}\")\n",
    "\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91937444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kalman():\n",
    "    kf = cv2.KalmanFilter(4, 2)               # 4 state vars (x, y, vx, vy), 2 measurements (x, y)\n",
    "    # State transition: x' = x + vx, y' = y + vy\n",
    "    kf.transitionMatrix = np.array([\n",
    "        [1, 0, 1, 0],\n",
    "        [0, 1, 0, 1],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ], np.float32)\n",
    "    # We directly observe x and y\n",
    "    kf.measurementMatrix = np.array([\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0]\n",
    "    ], np.float32)\n",
    "    # Tune noise covariances as needed\n",
    "    kf.processNoiseCov = np.eye(4, dtype=np.float32) * 1e-2\n",
    "    kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 1e-1\n",
    "    kf.errorCovPost = np.eye(4, dtype=np.float32)\n",
    "    return kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e5d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m-world.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71254cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_classes(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60b7301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    text_thickness=2,\n",
    "    text_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd2d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kalman_filters = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Descriptive update]\n",
      "You are in a room with a person and a bed. The person is moving slightly within the same area, while the bed remains stationary. The person seems to be pacing around the same spot and the bed is situated a short distance away from them. Overall, the environment is fairly stable with minimal activity.\n",
      "\n",
      "[GPT Error] There is no current event loop in thread 'Thread-3 (send_to_gpt)'.\n",
      "\n",
      "[Descriptive update]\n",
      "In the room, there's a person who seems to be moving around slightly but generally staying in the same area. The bed also appears to be stationary in the room. A cell phone is also present and it is moving gradually around the room.\n",
      "\n",
      "[GPT Error] There is no current event loop in thread 'Thread-3 (send_to_gpt)'.\n",
      "[GPT Error] Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-BbAGzyIh3vSGGQfnsIr8cLe4 on tokens per min (TPM): Limit 10000, Requested 18417. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPT Error] Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 12405 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "\n",
      "[Descriptive update]\n",
      "A person is seen moving slowly around a room with two beds. The person's movement is primarily along the x-axis with minor ups and downs along the y-axis. The first bed is almost stationary and positioned further along the y-axis, while the second bed is closer to the person's initial position. The person is moving around a lot, potentially navigating around the room or performing various tasks. The beds do not seem to change their positions significantly, indicating that they are likely large, heavy, and non-mobile.\n",
      "\n",
      "[GPT Error] There is no current event loop in thread 'Thread-3 (send_to_gpt)'.\n"
     ]
    }
   ],
   "source": [
    "# start the background sender\n",
    "threading.Thread(target=send_to_gpt, daemon=True).start()\n",
    "\n",
    "\n",
    "# ─── MAIN TRACK & KALMAN LOOP ───────────────────────────────────────────────────\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 1) Run Ultralytics tracking on THIS single frame\n",
    "    results = model.track(\n",
    "        frame, \n",
    "        tracker=\"custom_bytetrack.yaml\",\n",
    "        conf=0.25,\n",
    "        iou=0.5,\n",
    "        persist=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    result = results[0]  # model.track(frame) returns a list of 1 Result\n",
    "\n",
    "    annotated = frame.copy()\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    # 2) Loop through tracked boxes + IDs + Kalman as before\n",
    "    for box, cls, tid in zip(result.boxes.xyxy,\n",
    "                              result.boxes.cls,\n",
    "                              result.boxes.id):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "        # Kalman init/predict/correct\n",
    "        if tid not in kalman_filters:\n",
    "            kf = create_kalman()\n",
    "            kf.statePost = np.array([[cx],[cy],[0],[0]], np.float32)\n",
    "            kalman_filters[tid] = kf\n",
    "        else:\n",
    "            kf = kalman_filters[tid]\n",
    "\n",
    "        pred = kf.predict().flatten()\n",
    "        kf.correct(np.array([[cx],[cy]], np.float32))\n",
    "        px, py, pvx, pvy = pred\n",
    "\n",
    "        # draw detection & ID\n",
    "        label = f\"{result.names[int(cls)]} ID:{int(tid)}\"\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(annotated, label, (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "        # draw Kalman prediction\n",
    "        cv2.circle(annotated, (int(px), int(py)), 4, (0,0,255), -1)\n",
    "        cv2.arrowedLine(annotated,\n",
    "                        (int(cx), int(cy)),\n",
    "                        (int(px), int(py)),\n",
    "                        (0,0,255), 2, tipLength=0.3)\n",
    "\n",
    "        # buffer for GPT…\n",
    "        with buffer_lock:\n",
    "            scene_buffer.append({\n",
    "                \"track_id\": tid,\n",
    "                \"class\":    result.names[int(cls)],\n",
    "                \"cx\":       cx,\n",
    "                \"cy\":       cy,\n",
    "                \"vx\":       pvx,\n",
    "                \"vy\":       pvy\n",
    "            })\n",
    "\n",
    "    # 3) Display\n",
    "    cv2.imshow(\"YOLOv8 + ByteTrack + Kalman\", annotated)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byakugan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
